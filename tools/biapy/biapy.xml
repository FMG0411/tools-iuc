<tool id="biapy" name="Build a workflow with BiaPy" version="@TOOL_VERSION@+galaxy@VERSION_SUFFIX@" profile="@PROFILE@" license="MIT">
    <description>: Accessible deep learning on bioimages</description>
    <macros>
        <import>macros.xml</import>
    </macros>
    <edam_topics>
        <edam_topic>topic_3474</edam_topic>  <!-- Machine learning -->
        <!-- <edam_topic>topic_3753</edam_topic>  Deep learning (not in EDAM) -->
    </edam_topics>
    <edam_operations>
        <edam_operation>operation_2945</edam_operation>  <!-- Image segmentation -->
        <edam_operation>operation_3925</edam_operation>  <!-- Object detection -->
        <edam_operation>operation_3443</edam_operation>  <!-- Image denoising -->
        <!-- <edam_operation>Single image super-resolution (not in EDAM)</edam_operation> -->
        <edam_operation>operation_2946</edam_operation>  <!-- Image restoration -->
        <!-- <edam_operation>Image-to-image translation (not in EDAM)</edam_operation>   -->
        <!-- <edam_operation>operation_3442</edam_operation> Image classification -->
        <!-- <edam_operation>Self-supervision learning (not in EDAM)</edam_operation>  -->
        <edam_operation>operation_2944</edam_operation>  <!-- Image analysis -->
    </edam_operations>
    <requirements>
        <requirement type="package" version="3.6.2">biapy</requirement>
    </requirements>
    <expand macro="creators" />
    <stdio>
        <exit_code range=":-1" level="fatal" description="Error occurred. Please check Tool Standard Error"/>
        <exit_code range="137" level="fatal_oom" description="Out of Memory"/>
        <exit_code range="1:" level="fatal" description="Error occurred. Please check Tool Standard Error"/>
    </stdio>
    <command>
        <![CDATA[
        ## Define output directory
        mkdir -p results/$result_dir &&

        ########## Create yaml file ##########
        #if $mode_selection.selected_mode == 'create_new_cfg':
            python create_yaml.py
                --config_path '$mode_selection.selected_mode.cfg_config_process.config_path'
                --workflow '$mode_selection.selected_mode.cfg_config_process.workflow'
                --dims '$mode_selection.selected_mode.cfg_config_process.dim_and_obj.dimensionality.is_3d'
                --obj_slices '$mode_selection.selected_mode.cfg_config_process.dim_and_obj.dimensionality.obj_slices'
                --obj_size '$mode_selection.selected_mode.cfg_config_process.dim_and_obj.obj_size'
                #if $mode_selection.selected_mode.cfg_config_process.pretrained_model.model_source == 'biapy':
                    --model_source 'biapy'
                #elif $mode_selection.selected_mode.cfg_config_process.pretrained_model.model_source == 'biapy_pretrained':
                    --model '$mode_selection.selected_mode.cfg_config_process.pretrained_model.biapy_model_path'
                    --model_source 'biapy'
                #elif $mode_selection.selected_mode.cfg_config_process.pretrained_model.model_source == 'bmz_torchvision':
                    #if $mode_selection.selected_mode.cfg_config_process.pretrained_model.model_source.bmz_torchvision.bmz_or_torchvision == 'bmz'
                        --model_source 'bmz'
                        --model '$mode_selection.selected_mode.cfg_config_process.pretrained_model.model_source.bmz_torchvision.bmz_or_torchvision.bmz_model_name'
                    #else:
                        --model_source 'torchvision'
                        --model '$mode_selection.selected_mode.cfg_config_process.pretrained_model.model_source.bmz_torchvision.bmz_or_torchvision.torchvision_model_name'
                #end if
                #if $mode_selection.selected_mode.cfg_config_process.phase_decision.phases == 'train_test':
                    --train_raw_path '$mode_selection.selected_mode.cfg_config_process.phase_decision.train_sec.train_raw_path'
                    --train_gt_path '$mode_selection.selected_mode.cfg_config_process.phase_decision.train_sec.train_gt_path'
                    --test_raw_path '$mode_selection.selected_mode.cfg_config_process.phase_decision.test_sec.test_raw_path'
                    #if $mode_selection.selected_mode.cfg_config_process.phase_decision.test_sec.test_gt_avail == 'test_gt_yes':
                        --test_gt_path '$mode_selection.selected_mode.cfg_config_process.phase_decision.test_sec.test_gt_path'
                    #end if
                #end if
            &&

            #set config_file = '$mode_selection.selected_mode.cfg_config_process.config_path'
        #else
            #set config_file = '$mode_selection.selected_mode.config_path'
        #end if

        ########## Run BiaPy ##########
        biapy 
            --config '$config_file'
            --result_dir 'results/$result_dir'
            --name '$name'
            --run_id '$run_id'
            --gpu '$gpu'
        &&

        ########## Compress output ##########
        tar -czf '$results_archive' -C results "$result_dir"
        ]]>
    </command>

    <inputs>
        <conditional name="mode_selection">
            <param name="selected_mode" type="select" label="Do you have a configuration file?">
                <option value="custom_cfg" selected="true">
                    Yes, I already have one and I want to run BiaPy directly.
                </option>
                <option value="create_new_cfg">
                    No, I want to create one from scratch.
                </option>
            </param>
            <when value="custom_cfg">
                <param name="config_path" type="file" label="Select a configuration file" help="Input configuration file"/>
            </when>
            <when value="create_new_cfg">
                <section name="cfg_config_process" title="Workflow configuration process" expanded="True">
                    <!-- Q0 -->
                    <param name="config_path" type="text" label="Select a path to store the configuration file to be created" help="Path to store the configuration file to be created"/>
                    
                    <section name="dim_and_obj" title="Dimensionality and target object measures" expanded="True">    
                        <conditional name="dimensionality">
                            <!-- Q1 -->
                            <param name="is_3d" type="select" label="Are your images in 3D?" help="The purpose of this question is to determine the type of images you will use. This will help decide which deep learning model will be applied to process your images. If you select 'No', 2D images are expected, e.g. those that can be represented as (x, y, channels), for example, (512, 1024, 2). If 'Yes', 3D images are expected, e.g. those that can be represented as (x, y, z, channels), for example, (400, 400, 50, 1). The last option, 'No, but I would like to have a 3D stack output', refers to working with 2D images. After processing them with the deep learning model, the images will be combined in sequence to create a 3D stack. This option is useful if your 2D images together form a larger 3D volume">
                                <option value="2d" selected="true">No</option>
                                <option value="3d">Yes</option>
                                <option value="2d_stack">No, but I would like to have a 3D stack output</option>
                            </param>
                            <when value="3d">
                                <!-- Q7 -->
                                <param name="obj_slices" type="select" label="How many slices can an object be represented in?" help="This question aims to determine the size of the objects of interest in your images along the Z axis. For example, if you're working on cell nucleus segmentation, you should have a rough idea of how many slices the nuclei will appear across in the images. Refer to the image below for a clearer, visual understanding of this concept">
                                    <option value="1-5" selected="true">1-5 slices</option>
                                    <option value="5-10">5-10 slices</option>
                                    <option value="10-20">10-20 slices</option>
                                    <option value="20-60">20-60 slices</option>
                                    <option value="60+">More than 60 slices</option>
                                </param>
                            </when>
                            <when value="2d"/>
                            <when value="2d_stack"/>
                        </conditional>
                        <!-- Q6 -->
                        <param name="obj_size" type="select" label="What is the average object width/height in pixels?" help="This question is meant to determine the size of the objects of interest in your images. For example, if you're working on cell nucleus segmentation, you should have a general idea of the size these nuclei will appear in the images. It does not need to be super accurate, a rough estimation will be ok">
                            <option value="0-25" selected="true">0-25 px</option>
                            <option value="25-100">25-100 px</option>
                            <option value="100-200">100-200 px</option>
                            <option value="200-500">200-500 px</option>
                            <option value="500+">More than 500 px</option>
                        </param>
                    </section>

                    <!-- Q2 -->
                    <param name="workflow" type="select" label="Do you want to:" help="Select a workflow to run. We refer the reader to the following link of our documentation with a further explanation: https://biapy.readthedocs.io/en/latest/get_started/select_workflow.html">
                        <option value="semantic" selected="true">Generate masks of different (or just one) objects/regions within the image</option>
                        <option value="instance">Generate masks for each object in the image</option>
                        <option value="detection">Count/locate roundish objects within the images (don’t care of the exact mask that circumscribes the objects)</option>
                        <option value="denoising">Clean noisy images</option>
                        <option value="sr">Upsample images into higher resolution</option>
                        <option value="cls">Assign a label to each image</option>
                        <option value="sr2">Restore a degraded image</option>
                        <option value="i2i">Generate new images based on an input one</option>
                    </param>

                    <conditional name="pretrained_model">
                        <!-- Q3 -->
                        <param name="model_source" type="select" label="Do you want to use a pre-trained model?" help="This question determines how the deep learning model will be built. Based on your choice, additional questions may appear to guide the model setup process. Before using a deep learning model, it must be trained. The idea is to train the model on a specific task and then apply it to new images. For example, you could train a model to classify images based on their labels. If the training is successful, the model should be able to classify new images automatically. Training is a crucial step to ensure good results, but it requires labeled data, also known as 'ground truth'. Using the image classification example, you would need to label the training images first, like 'image1.png' as class 1, 'image2.png' as class 2, etc. Because training can be time-consuming, using a pre-trained model can be a major advantage. A pre-trained model has already been trained on a dataset and has some level of knowledge. If your images are similar to those used to train the pre-trained model, you might get good results without retraining it. If not, having similar images will at least make the retraining process faster and require fewer images. That's why it’s always a good idea to check for available pre-trained models that match your needs. Here are the options available: 1) 'No, I want to build a model from scratch'. This option configures BiaPy to create a model from the ground up. No additional input will be required, and BiaPy will automatically configure the model based on the selected workflow and image dimensions. 2) 'Yes, I have a model previously trained in BiaPy'. Choose this option if you already have a model trained with BiaPy. The model must be stored in the folder you’ve selected for saving results, specifically in the 'checkpoints' folder, as a file with a '.pth' extension. You’ll need to specify this file when prompted after selecting this option. 3) 'Yes, I want to check if there is a pre-trained model I can use'. This option allows you to load a pre-trained model from an external source. Currently, BiaPy supports loading models from the BioImage Model Zoo and Torchvision. Depending on the workflow and image dimensions, you can search for compatible models to use">
                            <option value="biapy" selected="true">No, I want to build a model from scratch</option>
                            <option value="biapy_pretrained" selected="true">Yes, I have a model previously trained in BiaPy</option>
                            <option value="bmz_torchvision" selected="true">Yes, I want to check if there is a pre-trained model I can use</option>
                        </param>
                        <when value="biapy_pretrained">
                            <!-- Q4 -->
                            <param name="biapy_model_path" type="data" format="data" label="Select the model trained with BiaPy before" help="This question is about setting the path to a pre-trained model trained before using BiaPy. The model should be stored in the folder you previously selected for saving results. Specifically, in the 'checkpoints' folder, there will be a file with a '.pth' extension, which represents the checkpoint of the pre-trained model"/>
                        </when>
                        <when value="bmz_torchvision">
                            <!-- Q5 -->
                            <conditional name="bmz_torchvision_model">
                                <param name="bmz_or_torchvision" type="select" label="Which is the source of the model?" help="Enter the source of the model, whether if it is available through the BioImage Model Zoo or TorchVision">
                                    <option value="bmz" selected="true">BioImage Model Zoo</option>
                                    <option value="torchvision" selected="true">TorchVision</option>
                                </param>
                                <when value="bmz">
                                    <param name="bmz_model_name" type="data" format="data" label="BioImage Model Zoo model name" help="Enter the name of the pre-trained model from the BioImage Model Zoo (BMZ). You can explore BMZ available models at https://bioimage.io/#/models. To filter models compatible with BiaPy, click on the BiaPy icon. Make sure the selected model matches your chosen dimensionality (2D or 3D) and is appropriate for your intended workflow. For example, if you're running a semantic segmentation workflow, the model must be pre-trained for that specific task—this information can be verified by checking the model’s tags."/>
                                </when>
                                <when value="torchvision">
                                    <param name="torchvision_model_name" type="data" format="data" label="BioImage Model Zoo model name" help="Enter the name of the pre-trained model from TorchVision. You can check in https://docs.pytorch.org/vision/0.21/models.html#general-information-on-pre-trained-weights, e.g. 'alexnet' for classification"/>
                                </when>
                            </conditional>
                        </when>  
                        <when value="biapy"/>
                    </conditional>
                       
                    <conditional name="phase_decision">
                        <!-- Q8 -->
                        <param name="phases" type="select" label="What do you want to do?" help="This question is meant to determine which phases of the workflow you want to perform. Before using a deep learning model, it needs to be trained. The goal is to train the model on a specific task, then apply it to new images. For instance, you might train a model to classify images based on their labels. If the training is successful, the model should be able to classify new images automatically. This final phase is known as 'Test', sometimes referred to as 'Inference' or 'Prediction', which all describe the process of applying the model's knowledge to new data">
                            <option value="train_test" selected="true">Train and test a model</option>
                            <option value="train">Train a model</option>
                            <option value="test">Test a model</option>
                        </param>
                        <when value="train_test">
                            <section name="train_sec" title="Train data" expanded="True"> 
                                <!-- Q9 -->
                                <param name="train_raw_path" type="data" format="txt" label="Specify the location of the training raw image folder" help="In this step, you need to specify the folder where the images for training the network are located. All these images must have the same number of channels. It's also important that each channel contains the same type of information to avoid confusing the model. A more detailed description can be found in BiaPy documentation: https://biapy.readthedocs.io/en/latest/get_started/how_it_works.html"/>
                                <!-- Q10 -->
                                <param name="train_gt_path" type="data" format="txt" label="Specify the location of the training ground truth (target) folder" help="In this step, you need to specify the directory where the training target data, also known as 'ground truth', is located. This target data will be used to train the network. The target vary depending on the workflow: For Semantic segmentation, single-channel images are expected, specifically semantic masks. In these masks, each pixel will have a specific integer value corresponding to the class it belongs to. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. Additionally, the number of classes to predict will be automatically extracted based on the different classes found in the images. For Instance segmentation, one or two-channel images are expected. The first channel must always contain the instance masks, where each pixel has an integer value representing the object it belongs to. If two channels are provided, the second channel should contain the class of each instance. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. If two channels are present, as in semantic segmentation, the number of classes to predict will be automatically extracted from the different classes found. For Detection, files with the coordinates of object centers (centroids) are expected. The checks include: 1) ensuring the files can be read correctly; 2) verifying that the required columns for creating the coordinates are present in each file. Additionally, if the files contain a 'class' column, the number of classes will automatically be set to the highest value found in that column across all files. For Super-resolution, high-resolution images are expected, meaning these images will be 2x or 4x larger than the versions in the 'Train data (raw)' folder from the previous step. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For example, if the images are uint8, their values should be between 0 and 255 across all images. For Image-to-Image, images are expected. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For instance, if the images are uint8, the values should range between 0 and 255 across all images. You can find more information about each workflow and examples of dataset in BiaPy documentation: https://biapy.readthedocs.io/en/latest/get_started/select_workflow.html"/>
                            </section> 
                            <section name="test_sec" title="Test data" expanded="True"> 
                                <!-- Q11 -->
                                <param name="test_raw_path" type="data" format="txt" label="Specify the location of the test raw image folder" help="In this step, you need to specify the folder where the images for testing the network are located. All these images must have the same number of channels. It's also important that each channel contains the same type of information to avoid confusing the model"/>
                                <conditional name="test_data_avail">
                                    <!-- Q12 -->
                                    <param name="test_gt_avail" type="select" format="txt" label="Do you have test ground truth (target) data?" help="This question is meant to determine whether you have target data, or ground truth, for the test data. If you have this, BiaPy will be able to calculate various metrics, which will vary depending on the selected workflow, to measure the model's performance">
                                        <option value="test_gt_no" selected="true">No</option>
                                        <option value="test_gt_yes">Yes</option>
                                    </param>
                                    <when value="test_gt_yes">
                                        <!-- Q13 -->
                                        <param name="test_gt_path" type="data" format="txt" label="Specify the location of the test ground truth (target) folder?" help="In this step, you need to specify the directory where the test target data, also known as 'ground truth', is located. This target data will be used to test the network. The target vary depending on the workflow: For Semantic segmentation, single-channel images are expected, specifically semantic masks. In these masks, each pixel will have a specific integer value corresponding to the class it belongs to. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. Additionally, the number of classes to predict will be automatically extracted based on the different classes found in the images. For Instance segmentation, one or two-channel images are expected. The first channel must always contain the instance masks, where each pixel has an integer value representing the object it belongs to. If two channels are provided, the second channel should contain the class of each instance. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. If two channels are present, as in semantic segmentation, the number of classes to predict will be automatically extracted from the different classes found. For Detection, files with the coordinates of object centers (centroids) are expected. The checks include: 1) ensuring the files can be read correctly; 2) verifying that the required columns for creating the coordinates are present in each file. Additionally, if the files contain a 'class' column, the number of classes will automatically be set to the highest value found in that column across all files. For Super-resolution, high-resolution images are expected, meaning these images will be 2x or 4x larger than the versions in the 'Train data (raw)' folder from the previous step. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For example, if the images are uint8, their values should be between 0 and 255 across all images. For Image-to-Image, images are expected. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For instance, if the images are uint8, the values should range between 0 and 255 across all images. You can find more information about each workflow and examples of dataset in BiaPy documentation: https://biapy.readthedocs.io/en/latest/get_started/select_workflow.html"/>
                                    </when>
                                    <when value="test_gt_no"/>
                                </conditional>
                            </section> 
                        </when>

                        <when value="train">
                            <section name="train_sec" title="Train data" expanded="True"> 
                                <!-- Q9 (repeated)--> 
                                <param name="train_raw_path" type="data" format="txt" label="Specify the location of the training raw image folder" help="In this step, you need to specify the folder where the images for training the network are located. All these images must have the same number of channels. It's also important that each channel contains the same type of information to avoid confusing the model. A more detailed description can be found in BiaPy documentation: https://biapy.readthedocs.io/en/latest/get_started/how_it_works.html"/>
                                <!-- Q10 (repeated)-->
                                <param name="train_gt_path" type="data" format="txt" label="Specify the location of the training ground truth (target) folder" help="In this step, you need to specify the directory where the training target data, also known as 'ground truth', is located. This target data will be used to train the network. The target vary depending on the workflow: For Semantic segmentation, single-channel images are expected, specifically semantic masks. In these masks, each pixel will have a specific integer value corresponding to the class it belongs to. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. Additionally, the number of classes to predict will be automatically extracted based on the different classes found in the images. For Instance segmentation, one or two-channel images are expected. The first channel must always contain the instance masks, where each pixel has an integer value representing the object it belongs to. If two channels are provided, the second channel should contain the class of each instance. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. If two channels are present, as in semantic segmentation, the number of classes to predict will be automatically extracted from the different classes found. For Detection, files with the coordinates of object centers (centroids) are expected. The checks include: 1) ensuring the files can be read correctly; 2) verifying that the required columns for creating the coordinates are present in each file. Additionally, if the files contain a 'class' column, the number of classes will automatically be set to the highest value found in that column across all files. For Super-resolution, high-resolution images are expected, meaning these images will be 2x or 4x larger than the versions in the 'Train data (raw)' folder from the previous step. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For example, if the images are uint8, their values should be between 0 and 255 across all images. For Image-to-Image, images are expected. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For instance, if the images are uint8, the values should range between 0 and 255 across all images. You can find more information about each workflow and examples of dataset in BiaPy documentation: https://biapy.readthedocs.io/en/latest/get_started/select_workflow.html"/>
                            </section> 
                        </when>

                        <when value="test">
                            <section name="test_sec" title="Test data" expanded="True"> 
                                <!-- Q11 (repeated) -->
                                <param name="test_raw_path" type="data" format="txt" label="Specify the location of the test raw image folder" help="In this step, you need to specify the folder where the images for testing the network are located. All these images must have the same number of channels. It's also important that each channel contains the same type of information to avoid confusing the model"/>
                                <conditional name="test_data_avail">
                                    <!-- Q12 (repeated) -->
                                    <param name="test_gt_avail" type="select" format="txt" label="Do you have test ground truth (target) data?" help="This question is meant to determine whether you have target data, or ground truth, for the test data. If you have this, BiaPy will be able to calculate various metrics, which will vary depending on the selected workflow, to measure the model's performance">
                                        <option value="test_gt_no" selected="true">No</option>
                                        <option value="test_gt_yes">Yes</option>
                                    </param>
                                    <when value="test_gt_yes">
                                        <!-- Q13 (repeated) -->
                                        <param name="test_gt_path" type="data" format="txt" label="Specify the location of the test ground truth (target) folder?" help="In this step, you need to specify the directory where the test target data, also known as 'ground truth', is located. This target data will be used to test the network. The target vary depending on the workflow: For Semantic segmentation, single-channel images are expected, specifically semantic masks. In these masks, each pixel will have a specific integer value corresponding to the class it belongs to. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. Additionally, the number of classes to predict will be automatically extracted based on the different classes found in the images. For Instance segmentation, one or two-channel images are expected. The first channel must always contain the instance masks, where each pixel has an integer value representing the object it belongs to. If two channels are provided, the second channel should contain the class of each instance. The checks performed include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels. If two channels are present, as in semantic segmentation, the number of classes to predict will be automatically extracted from the different classes found. For Detection, files with the coordinates of object centers (centroids) are expected. The checks include: 1) ensuring the files can be read correctly; 2) verifying that the required columns for creating the coordinates are present in each file. Additionally, if the files contain a 'class' column, the number of classes will automatically be set to the highest value found in that column across all files. For Super-resolution, high-resolution images are expected, meaning these images will be 2x or 4x larger than the versions in the 'Train data (raw)' folder from the previous step. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For example, if the images are uint8, their values should be between 0 and 255 across all images. For Image-to-Image, images are expected. The checks include: 1) ensuring all images can be read correctly; 2) verifying that all images have the same number of input channels; 3) checking that all images are within the same value range. For instance, if the images are uint8, the values should range between 0 and 255 across all images. You can find more information about each workflow and examples of dataset in BiaPy documentation: https://biapy.readthedocs.io/en/latest/get_started/select_workflow.html"/>
                                    </when>
                                    <when value="test_gt_no"/>
                                </conditional>
                            </section> 
                        </when>
                    </conditional>

                </section>
            </when>
        </conditional>
        <section name="general_opts" title="General options" expanded="True">
            <param name="result_dir" type="text" label="Select the output directory" help="A directory path where all the workflow files will be stored. Just a folder name, not a full path"/>
            <param name="name" type="text" label="Select a name for the workflow" help="Just a name for the job. E.g. 'my_first_semantic_segmentation'"/>
        </section>
        <section name="advanced_opts" title="Advanced options" expanded="False">
            <param name="run_id" type="integer" value="1" min="1" max="100" optional="true" label="ID" help="Number that should be increased when one need to run the same job multiple times (reproducibility)"/>
            <param name="gpu" type="text" optional="true" label="Select a GPU to use" help="Number of the GPU to run the job in"/>
        </section>
    </inputs>
    <outputs>
        <data name="results_archive" format="tar.gz" label="BiaPy workflow results"/>
    </outputs>
    <tests>
        <test>
            <param name="selected_mode" value="custom_cfg"/>
            <param name="config_path" value="example.yaml"/>
            <param name="result_dir" value="test_results"/>
            <param name="name" value="my_experiment"/>
            <output name="results_archive" file="my_experiment_example.tar.tar.gz">
                <assert_contents>
                    <has_archive_member path="test_results/my_experiment_example/checkpoints/my_experiment_example_1-checkpoint-best.pth"/>
                    <has_archive_member path="test_results/my_experiment_example/results/my_experiment_example_1/test_results_metrics"/>
                    <has_archive_member path="test_results/my_experiment_example/results/my_experiment_example_1/per_image/im_0000.tif"/>
                </assert_contents>
            </output>
        </test>
    </tests>
<help><![CDATA[
**What it does**

This tool runs a BiaPy workflow for image analysis using deep learning models. BiaPy is a bioimage analysis pipeline designed to simplify training, prediction, and evaluation across a variety of tasks such as image segmentation, classification, denoising, and more.

---

**Usage**

There are two main usage modes for this tool:

1. **Using a custom configuration file (YAML)**  
   If you already have a BiaPy configuration file, you can upload it directly. The tool will use this configuration without further modification to run the specified BiaPy workflow.

2. **Constructing a configuration interactively**  
   If you do not have a YAML configuration file, the tool can help you build one by asking a set of guided questions. This includes settings like:
   - Task type (e.g., segmentation, classification)
   - Model architecture
   - Input/output patch sizes
   - Data paths and formats
   - Training parameters (epochs, batch size, etc.)

Once these options are specified, the tool generates a valid BiaPy YAML config and proceeds to execute the workflow.

---

**Output**

The output depends on the chosen workflow and may include:
- Trained model weights
- Prediction results
- Evaluation metrics (if ground truth is available)
- Log files and training history

---

**Tips**

- For best results, ensure that your input data format matches what BiaPy expects. Refer to the [BiaPy documentation](https://biapy.readthedocs.io/en/latest/) for details.
- Use the "interactive mode" if you're new to BiaPy and want guidance on configuration.
- Advanced users with pre-tuned configs may prefer uploading a YAML directly for faster execution.

---

**References**

- BiaPy documentation: https://biapy.readthedocs.io/
- Galaxy Tool Development: https://galaxyproject.org/tools/
]]></help>
    <expand macro="citations"/>
</tool>